{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "LTSbj-pAFm4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from sklearn.metrics import classification_report\n",
        "import csv\n",
        "import json"
      ],
      "metadata": {
        "id": "-7_hRdeb1iT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"hackathon-pln-es/twitter_sexismo-finetuned-robertuito-exist2021\")"
      ],
      "metadata": {
        "id": "_j6GUW8aGDU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_es = pd.read_csv(\"labeled_train_es.csv\")\n",
        "dev_data_es = pd.read_csv(\"labeled_dev_es.csv\")"
      ],
      "metadata": {
        "id": "iikjj9TFGTgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"translated_tweet\"], padding=\"max_length\", truncation=True, max_length=130)"
      ],
      "metadata": {
        "id": "iangUfqB1kuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(train_data_es)\n",
        "dev_dataset = Dataset.from_pandas(dev_data_es)"
      ],
      "metadata": {
        "id": "usY5MeTKL1wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_data = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dev_data = dev_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "W8BVzxlQH7mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"hackathon-pln-es/twitter_sexismo-finetuned-robertuito-exist2021\",\n",
        "                                                           num_labels=2)"
      ],
      "metadata": {
        "id": "maLu9lv8I-Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"leslyarun/fbeta_score\")"
      ],
      "metadata": {
        "id": "kWNJYuplNfNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    report = classification_report(labels, predictions, output_dict=True)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"f1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"precision\": report[\"macro avg\"][\"precision\"],\n",
        "        \"recall\": report[\"macro avg\"][\"recall\"],\n",
        "        \"f2\": metric.compute(predictions=predictions, references=labels, beta=2),\n",
        "    }"
      ],
      "metadata": {
        "id": "UiSbJNaHOByR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  num_train_epochs=1)"
      ],
      "metadata": {
        "id": "RrKNEkQpOPNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_data,\n",
        "    eval_dataset=tokenized_dev_data,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "vnJ5MkyZOTmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "TDcRPs6yPbPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "xxulVlZPiKlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"output\")"
      ],
      "metadata": {
        "id": "SE4Lgwgaqlnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model you previously trained\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"output\")\n",
        "\n",
        "# init trainer\n",
        "trainer = Trainer(\n",
        "              model = model)"
      ],
      "metadata": {
        "id": "HdP0auhN9jat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "KbMrJUO3iV-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to classify a tweet and return the result in the desired format\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    encoding = tokenizer(tweet, return_tensors=\"pt\")\n",
        "\n",
        "    encoded_input_trc={}\n",
        "    for k,v in encoding.items():\n",
        "        v_truncated = v[:,:128]\n",
        "        encoded_input_trc[k]=v_truncated.to(trainer.model.device)\n",
        "    \n",
        "    outputs = trainer.model(**encoded_input_trc)\n",
        "    probas = outputs.logits.softmax(dim=-1)\n",
        "    predicted_label = probas.argmax().item()\n",
        "    soft_label = {\"YES\": probas[0, 1].item(), \"NO\": probas[0, 0].item()}\n",
        "    if predicted_label == 1:\n",
        "        hard_label = \"YES\"\n",
        "    else:\n",
        "        hard_label = \"NO\"\n",
        "    return {\"hard_label\": hard_label, \"soft_label\": soft_label}\n",
        "\n",
        "\n",
        "# Define function to classify all tweets in a csv file and return results in desired format\n",
        "def classify_csv(csv_file):\n",
        "    tweets = {}\n",
        "    with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            tweet_id, preprocessed_text, translated_tweet, label = row\n",
        "            tweets[tweet_id] = translated_tweet\n",
        "    results = {}\n",
        "    for tweet_id, translated_tweet in tweets.items():\n",
        "        result = classify_tweet(translated_tweet)\n",
        "        results[tweet_id] = result\n",
        "    return results\n",
        "\n",
        "\n",
        "# Classify train, dev, and test data and save results in json files\n",
        "train_results = classify_csv(\"labeled_train_es.csv\")\n",
        "print('train completed')\n",
        "dev_results = classify_csv(\"labeled_dev_es.csv\")\n",
        "print('dev completed')\n",
        "\n",
        "with open(\"train_results.json\", \"w\") as f:\n",
        "    json.dump(train_results, f)\n",
        "with open(\"dev_results.json\", \"w\") as f:\n",
        "    json.dump(dev_results, f)\n"
      ],
      "metadata": {
        "id": "01CmYuMcS9cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to classify a tweet and return the result in the desired format\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    encoding = tokenizer(tweet, return_tensors=\"pt\")\n",
        "\n",
        "    encoded_input_trc={}\n",
        "    for k,v in encoding.items():\n",
        "        v_truncated = v[:,:128]\n",
        "        encoded_input_trc[k]=v_truncated.to(trainer.model.device)\n",
        "    \n",
        "    \n",
        "    outputs = trainer.model(**encoded_input_trc)\n",
        "    probas = outputs.logits.softmax(dim=-1)\n",
        "    predicted_label = probas.argmax().item()\n",
        "    soft_label = {\"YES\": probas[0, 1].item(), \"NO\": probas[0, 0].item()}\n",
        "    if predicted_label == 1:\n",
        "        hard_label = \"YES\"\n",
        "    else:\n",
        "        hard_label = \"NO\"\n",
        "    return {\"hard_label\": hard_label, \"soft_label\": soft_label}\n",
        "\n",
        "\n",
        "# Define function to classify all tweets in a csv file and return results in desired format\n",
        "def classify_csv(csv_file):\n",
        "    tweets = {}\n",
        "    with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader) # skip header row\n",
        "        for row in reader:\n",
        "            tweet_id, preprocessed_text, translated_tweet = row\n",
        "            tweets[tweet_id] = translated_tweet\n",
        "    results = {}\n",
        "    for tweet_id, translated_tweet in tweets.items():\n",
        "        result = classify_tweet(translated_tweet)\n",
        "        results[tweet_id] = result\n",
        "    return results\n",
        "    \n",
        "\n",
        "test_results = classify_csv(\"translated_test_es.csv\")\n",
        "\n",
        "with open(\"test_results.json\", \"w\") as f:\n",
        "    json.dump(test_results, f)"
      ],
      "metadata": {
        "id": "EP0vPaFv_bj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}